```{r}
#Loading the Data

rm(list=ls())
data<- read.csv("https://raw.githubusercontent.com/rajdeepsaha0809/Nonpartametric-Inference-Project/main/framingham%20heart%20disease%20dataset.csv")
head(data)
dim(data)
str(data)
```
```{r}
#checking for missing values
sum(is.na(data))
cbind(lapply(lapply(data, is.na), sum))
library(mice)
md.pattern(data)
library(VIM)
aggr_plot <- aggr(data, col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, labels=names(data), cex.axis=.7, gap=3, ylab=c("Histogram of missing data","Pattern"))
marginplot(data[c(3,15)])
```
```{r}
#Imputing the missing data
library(Rcpp)
tempData <- mice(data, m=5, maxit= 50, meth= 'pmm', seed= 500)
summary(tempData)
Data <- complete(tempData, 1)
sum(is.na(Data))
```
```{r}
#Check for Data Imbalance
attach(Data)
sum(TenYearCHD == 1) / nrow(Data)
sum(TenYearCHD == 0) / nrow(Data)
library(ROSE)
newData <- ovun.sample(TenYearCHD~., data = Data, method = "over", N = 7000)$data
attach(newData)
sum(TenYearCHD == 1) / nrow(newData)
sum(TenYearCHD == 0) / nrow(newData)
```
```{r}
#Feature Selection
library(leaps)
regfit.full = regsubsets(TenYearCHD~., data = newData, nvmax = 16)
reg.summary = summary(regfit.full)
reg.summary
which.min(reg.summary$bic)
plot(reg.summary$bic, xlab = "No. of Variables", ylab = "BIC", types = 'l')
points(9, reg.summary$bic[9], col= "blue", cex=1.5, pch = 8)
coef(regfit.full, 9)
```
```{r}
#Final Data for Analysis
names(coef(regfit.full, 9))[-1]
finalData = newData[,-c(4, 6, 9, 12, 13, 14)]
dim(finalData)
finalData$TenYearCHD <- as.factor(finalData$TenYearCHD)
```
```{r}
#Splitting the Data
set.seed(2022)
index1 = sample(1:nrow(finalData),floor(0.85*nrow(finalData)))
train = finalData[index1, ]
remaining = finalData[-index1, ]
index2 = sample(1:nrow(remaining),floor(2/3*nrow(remaining)))
crossval = remaining[index2, ]
test = remaining[-index2, ]
actual_TenYearCHD=crossval$TenYearCHD
dim(train)
dim(crossval)
dim(test)
```
```{r}
library(tibble)
library(cvms)
f_cfm <- function(x){
  cfm <- as.tibble(x)
  cname <- colnames(cfm)
  print(plot_confusion_matrix(cfm, target_col = cname[2], prediction_col =  cname[1], counts_col = cname[3]))
}
f_bar <- function(pred, act){
  d1 <- as.matrix(data.frame(as.vector(table(pred)), as.vector(table(act))))
  colnames(d1) <- c("Predicted","Actual")
  rownames(d1) <- c("1","0")
  barplot(d1, main="Ratio of 1 and 0 in Predicted and Actual",col=c("pink","cyan"))
  legend("topright",c("0","1"),fill=c("pink","cyan")) 
}
```

```{r}
#Logistic Regression
attach(finalData)
TenYearCHD <- as.factor(TenYearCHD)
threshold <- seq(0.1, 0.9, 0.01)
fscore <- array(0)
for(i in 1:length(threshold)){
  logistic.fit <- glm(TenYearCHD~., data = train, family = binomial)
  logistic.probs <- predict(logistic.fit, crossval, type = "response")
  logistic.pred <- rep("0", nrow(crossval))
  logistic.pred[logistic.probs > threshold[i]]= "1"
  tab <- table(logistic.pred, crossval$TenYearCHD)
  prec <- tab[2,2]/(tab[2,2] + tab[2,1])
  #print(prec)
  recall <- tab[2,2]/(tab[2,2] + tab[1,2])
  #print(recall)
  fscore[i] <- (2*prec*recall)/(prec + recall)
}
data.frame(threshold, fscore)
max_acc <- which.max(fscore)
paste("Maximum F1-score is for thresold value of ", threshold[max_acc], " and is = ",round(fscore[max_acc],4))

logistic.fit <- glm(TenYearCHD~., data = train, family = binomial)
logistic.probs <- predict(logistic.fit, crossval, type = "response")
logistic.pred <- rep(0, nrow(crossval))
logistic.pred[logistic.probs > threshold[max_acc]] = 1
actual <- crossval$TenYearCHD
logistic_table <- table(logistic.pred, actual)
f_cfm(logistic_table)
f_bar(logistic.pred, crossval$TenYearCHD)
```
```{r}
library(tree)
tree.fit <- tree(TenYearCHD~., data= train)
set.seed(2022)
cv.tenyearchd <- cv.tree(tree.fit, FUN= prune.misclass)
cv.tenyearchd #dev corresponds to misclassification error rate
par(mfrow=c(1,2))
plot(cv.tenyearchd$size, cv.tenyearchd$dev, type= "b")
plot(cv.tenyearchd$k, cv.tenyearchd$dev, type= "b")
prune.tree <- prune.misclass(tree.fit, best = 4)
plot(prune.tree)
text(prune.tree, pretty = 0)
tree.pred <- predict(prune.tree, crossval, type = "class")
tree_table <- table(tree.pred, actual)
f_cfm(tree_table)

```
```{r}
#Random Forest
set.seed(1)
library(randomForest)
used_pred = floor(ncol(finalData))
rf.fit <- randomForest(TenYearCHD~., data = train, mtry = used_pred,
           importance = TRUE, maxdepth = 8)
rf.pred <- predict(rf.fit, newdata = crossval)
rf_table <- table(rf.pred, actual)
f_cfm(rf_table)
```
```{r}
misclassification_rate_logistic=(mean(logistic.pred!=actual))*100
misclassification_rate_tree=(mean(tree.pred!=actual))*100
misclassification_rate_forest=(mean(rf.pred!=actual))*100
paste("Misclassification Error Rate for Logistic Regression is",round(misclassification_rate_logistic,2),"%")
paste("Misclassification Error Rate for Decision Tree is",round(misclassification_rate_tree,2),"%")
paste("Misclassification Error Rate for Random Forest is",round(misclassification_rate_forest,2),"%")

```
```{r}
library(pROC)

#Logistic ROC
logistic.predict= predict(logistic.fit, crossval ,type="response")
ROC= roc(crossval$TenYearCHD, logistic.predict)
plot(ROC, col="blue")
paste("Area under the curve is",round(auc(ROC),2))

#Tree ROC
tree.predict=predict(tree.fit, crossval, type="vector")
ROC=roc(crossval$TenYearCHD, tree.predict[,2])
plot(ROC, col="blue")
paste("Area under the curve is",round(auc(ROC),2))

#Random Forest ROC
rf.predict=predict(rf.fit, crossval, type="prob")
ROC=roc(crossval$TenYearCHD, rf.predict[,2])
plot(ROC, col="blue")
paste("Area under the curve is",round(auc(ROC),2))
```

